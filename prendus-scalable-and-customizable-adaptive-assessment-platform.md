# Prendus Technology Overview Series
## Scalable and Customizable Adaptive Assessment Platform
### Jordan Last (insert other authors here)

### Abstract

One-on-one tutoring is an extremely effective learning intervention, but does not scale
well. Adaptive assessment platforms are an attempt to replicate the benefits of one-on-one
tutoring for each individual learner. We believe that existing adaptive assessment platforms,
while successful for specific subjects, lack key components that hinder their scalability into new
content areas. These components are (1) access to an unlimited number of high quality and
relevant assessment items and (2) a sufficiently detailed taxonomy of educational concepts
across all content areas. We aim to build an adaptive platform that utilizes these missing
components to scale and customize to the needs of any individual learner for any concept. We
will accomplish this by empowering students to create, review, answer, and share assessments.
These crowdsourced activities will lay the foundation for an unbounded repository of questions
(across all subject areas and grade levels) and a living taxonomy of concepts (constantly
changing and improving), which in turn, lay the foundation for an effective adaptive assessment
platform across content areas and grade levels. By the end of Phase I, we will have built and
validated a prototype containing basic versions of the major components we believe necessary
for this platform to succeed. 

### Problem

Learning in a group setting can be challenging. Each student learns at a
different pace and responds to learning materials uniquely. Because of this, it can be difficult for
a single instructor to tailor instruction time and material to each student’s needs unless the
instructor has only a few students. Unfortunately, this kind of one-on-one tutoring does not scale
well when there are far more learners than available instructors.

In 1984 Bloom published a paper reporting that the average tutored student performed
much better than the average student taught conventionally (Bloom, 1984). Because the
personalization of the learning seemed to cause such a dramatic benefit for the learner, the goal
of the paper was to find methods of group instruction that could be as effective as one-on-one
instruction. Despite the many advances in education technology and instruction since Bloom’s
paper, solutions based on its findings still fail to achieve widespread adoption.

Personalized learning with a tutor is expensive, so it is natural to turn to technology to
make the method cost-effective. Even though there have been many instances of personalized
learning systems (e.g. intelligent tutors, automated feedback systems, educational recommender
systems), these systems only focus on select areas and grade levels to varying degrees of
success. These systems do not scale to the needs of all individual learners for all concepts.

We propose an online and learner-sourced adaptive assessment platform that scales and
customizes to fit the needs of each individual learner for any concept. Bloom was searching for
methods as effective as one-to-one tutoring, and this platform could be one of those methods. 

### Overview of Key Components

Developing this kind of adaptive assessment platform will require significant time and
effort. We foresee considerable research and development being necessary for the following
components of the platform:

#### Unbounded Repository of Questions

For our adaptive algorithms to suggest
appropriate questions to the student, we must have access to a large repository of questions. This
repository must be unbounded in the number of questions and subject matter. Organizations
often invest significant resources to create proprietary question banks. These question banks are
inherently limited by the cost necessary to create, curate, and manage them. To overcome this
limitation, we have created a crowdsourced question creation platform.

#### Determining Question Quality

In order for an unbounded repository of
questions to be useful, they will need to be of sufficient quality. Measuring question quality is a
task that requires skill and is difficult to perform. Because of this difficulty, we will incorporate
crowdsourcing and other systemic methods to help us determine question quality for our
crowdsourced question banks.

#### Classifying Questions by Concept/Construct

For our adaptive algorithms to suggest highly
relevant questions to the student, each question must be classified by the correct concept. Simple concept tagging by an individual student may not be sufficient, and other algorithmic or systemic
methods may be necessary.

#### Living Taxonomy of Concepts/Constructs

Both determining student competency
and guiding the student from assessment to assessment require understanding the relationships
between concepts. To best adapt to a student’s needs, the taxonomy of concepts should be as
complete as possible. Manually constructing the taxonomy could prove extremely challenging.
Our taxonomy will incorporate crowdsourcing to create a living structure, able to adapt and
change in response to user input and algorithmic analysis.

#### Determining Student Competency

Measuring student ability at a granular level
requires determining appropriate metrics and sophisticated analysis of those metrics. Instructors
often do not have the time nor the ability to perform this analysis for each student. Alleviating
instructors’ burden for creating and grading formative learning exercises would require
automating measures of student competency. Automating processes such as diagnostic
assessment and measuring creative assignments may prove difficult.

#### Adaptive Algorithms

All of the previous components provide the basis for
creating effective adaptive algorithms. This will be the basis for the personalized learning that
we plan to offer. Our adaptive algorithms will guide students from concept to concept and from
assessment to assessment based off of their previous performance. Identifying and perfecting
these adaptive algorithms will be a significant task.

### Unbounded Repository of Questions

### Determining Question Quality

### Classifying Questions by Concept/Construct

### Living Taxonomy of Concepts/Constructs

### Determining Student Competency

### Adaptive Algorithms
